# backend/tasks.py

"""
Celery g√∂revleri - Asenkron, periyodik i≈ülemler
"""

import logging
from datetime import datetime, timedelta
from sqlalchemy import func, and_
from sqlalchemy.orm import Session

from celery_config import app
from database import SessionLocal
import models

logger = logging.getLogger(__name__)


@app.task(name='tasks.update_industry_benchmarks', bind=True, max_retries=3)
def update_industry_benchmarks(self):
    """
    Haftada bir √ßalƒ±≈üan g√∂rev: IndustryTemplate'leri ger√ßek verilerle g√ºncelle
    
    Her sekt√∂r i√ßin:
    - Ortalama t√ºketim (kWh/√ßalƒ±≈üan)
    - En iyi %20'lik dilim (best_in_class)
    - Hesaplanacaƒüƒ± tarihten 30 g√ºn geriye bakacak
    """
    db = SessionLocal()
    try:
        logger.info("üîÑ Benchmark g√ºncelleme ba≈üladƒ±...")
        
        # T√ºm sekt√∂rleri al
        industry_templates = db.query(models.IndustryTemplate).all()
        
        updated_count = 0
        for template in industry_templates:
            try:
                # Bu sekt√∂r i√ßin ≈üirketleri bul
                companies_in_industry = db.query(models.Company).filter(
                    models.Company.industry_type == template.industry_type
                ).all()
                
                if not companies_in_industry:
                    logger.warning(f"‚ö†Ô∏è {template.industry_name} i√ßin ≈üirket yok")
                    continue
                
                # Son 30 g√ºnl√ºk elektrik t√ºketim verisini al (sim√ºlasyon deƒüil)
                cutoff_date = datetime.now().date() - timedelta(days=30)
                
                # Sadece ger√ßek veri (is_simulation=False) ve elektrik t√ºketimi
                electricity_data = db.query(
                    func.sum(models.ActivityData.quantity).label('total_kwh'),
                    func.count(models.ActivityData.id).label('data_points'),
                    models.Facility.company_id
                ).filter(
                    models.ActivityData.activity_type == models.ActivityType.electricity,
                    models.ActivityData.is_simulation == False,
                    models.ActivityData.start_date >= cutoff_date,
                    models.Facility.id == models.ActivityData.facility_id,
                    models.Facility.company_id.in_([c.id for c in companies_in_industry])
                ).group_by(models.Facility.company_id).all()
                
                if not electricity_data:
                    logger.warning(f"‚ö†Ô∏è {template.industry_name} i√ßin son 30 g√ºnde veri yok")
                    continue
                
                # T√ºketim deƒüerlerini hesapla
                consumptions = [float(d.total_kwh) for d in electricity_data if d.total_kwh]
                
                if consumptions:
                    # Ortalama
                    avg_consumption = sum(consumptions) / len(consumptions)
                    
                    # En iyi %20 (Sƒ±fƒ±rdan d√º≈ü√ºkleri filtrele)
                    sorted_consumptions = sorted(consumptions)
                    best_20_percent_threshold = sorted_consumptions[int(len(sorted_consumptions) * 0.2)]
                    
                    # Verileri g√ºncelle
                    template.average_electricity_kwh = avg_consumption
                    template.best_in_class_electricity_kwh = best_20_percent_threshold
                    
                    db.commit()
                    updated_count += 1
                    
                    logger.info(
                        f"‚úÖ {template.industry_name}: "
                        f"Ortalama={avg_consumption:.0f} kWh, "
                        f"Best %20={best_20_percent_threshold:.0f} kWh"
                    )
                    
            except Exception as e:
                logger.error(f"‚ùå {template.industry_name} g√ºncellenirken hata: {e}")
                continue
        
        logger.info(f"‚úÖ Benchmark g√ºncelleme tamamlandƒ±: {updated_count} sekt√∂r g√ºncellendi")
        return {"updated": updated_count, "timestamp": datetime.now().isoformat()}
        
    except Exception as exc:
        logger.error(f"‚ùå Benchmark g√∂revi hatasƒ±: {exc}")
        # Retry logic
        raise self.retry(exc=exc, countdown=300)  # 5 dakika sonra retry
        
    finally:
        db.close()


@app.task(name='tasks.detect_anomalies', bind=True, max_retries=2)
def detect_anomalies(self):
    """
    G√ºnl√ºk √ßalƒ±≈üan g√∂rev: Son eklenen verilerde anomali tespiti
    
    Her ≈üirket i√ßin:
    - Son 30 g√ºn√ºn ortalamasƒ±
    - Yeni eklenen veri ile kar≈üƒ±la≈ütƒ±r
    - ƒ∞statistiksel anormallik varsa bildir
    """
    db = SessionLocal()
    try:
        logger.info("üîç Anomali tespiti ba≈üladƒ±...")
        
        # Son 24 saatte eklenen ActivityData'larƒ± bul
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        new_activities = db.query(
            models.ActivityData
        ).filter(
            # Mevcut SQLAlchemy ORM yapƒ±sƒ±nda created_at olmayabilir
            # Alternatif olarak, is_simulation=False ve son g√ºncellemesi...
            models.ActivityData.is_simulation == False
        ).all()
        
        anomaly_count = 0
        
        for company in db.query(models.Company).all():
            try:
                # Bu ≈üirketin son 30 g√ºnl√ºk ortalamasƒ±nƒ± al
                cutoff_date = datetime.now().date() - timedelta(days=30)
                
                historical_data = db.query(
                    func.avg(models.ActivityData.quantity).label('avg_quantity')
                ).filter(
                    models.ActivityData.activity_type == models.ActivityType.electricity,
                    models.ActivityData.is_simulation == False,
                    models.ActivityData.start_date >= cutoff_date,
                    models.Facility.company_id == company.id,
                    models.Facility.id == models.ActivityData.facility_id
                ).scalar()
                
                if not historical_data:
                    continue
                
                # Son verileri kontrol et
                recent_data = db.query(models.ActivityData).filter(
                    models.ActivityData.activity_type == models.ActivityType.electricity,
                    models.ActivityData.is_simulation == False,
                    models.ActivityData.start_date >= cutoff_date,
                    models.Facility.company_id == company.id,
                    models.Facility.id == models.ActivityData.facility_id
                ).order_by(models.ActivityData.end_date.desc()).limit(1).first()
                
                if recent_data and historical_data:
                    # Anomali kontrol√º: %20 √ºzeri artƒ±≈ü
                    deviation = (recent_data.quantity - historical_data) / historical_data if historical_data > 0 else 0
                    
                    if deviation > 0.20:  # %20 √ºzeri artƒ±≈ü
                        logger.warning(
                            f"‚ö†Ô∏è ANOMALI BULUNDU: {company.name} - "
                            f"Elektrik t√ºketimi +{deviation*100:.1f}% arttƒ±"
                        )
                        anomaly_count += 1
                        
                        # YENƒ∞: Bildirim olu≈ütur
                        try:
                            from services.notification_service import get_notification_service
                            notif_service = get_notification_service()
                            
                            # ≈ûirket sahibine bildir
                            owner_id = company.owner_id
                            if owner_id:
                                notif_service.create_notification(
                                    db=db,
                                    user_id=owner_id,
                                    notification_type='anomaly',
                                    title=f"‚ö†Ô∏è {company.name}: Elektrik T√ºketimi Anormal!",
                                    message=f"Bu ay elektrik t√ºketimi ge√ßen aya g√∂re {deviation*100:.1f}% artmƒ±≈ü. "
                                            f"L√ºtfen kontrol ederek inceleyiniz.",
                                    company_id=company.id,
                                    action_url=f"/dashboard/companies/{company.id}/anomalies",
                                    send_email=True
                                )
                        except Exception as e:
                            logger.error(f"‚ùå Anomali bildirimi olu≈üturma hatasƒ±: {e}")
                        
            except Exception as e:
                logger.error(f"‚ùå {company.name} anomali kontrol√º hatasƒ±: {e}")
                continue
        
        logger.info(f"‚úÖ Anomali tespiti tamamlandƒ±: {anomaly_count} anomali bulundu")
        return {"anomalies_detected": anomaly_count, "timestamp": datetime.now().isoformat()}
        
    except Exception as exc:
        logger.error(f"‚ùå Anomali tespiti hatasƒ±: {exc}")
        raise self.retry(exc=exc, countdown=300)
        
    finally:
        db.close()


# Health check g√∂revi (i≈ületim kontrol√º i√ßin)
@app.task(name='tasks.health_check', bind=True)
def health_check(self):
    """
    Her saatte √ßalƒ±≈üan g√∂rev - Celery sistemi saƒülƒ±k durumunu kontrol et
    """
    logger.info(f"‚úÖ Celery health check: {datetime.now().isoformat()}")
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


# YENƒ∞: OCR ƒ∞≈üleme G√∂revi (Mod√ºl 2.2)
@app.task(name='tasks.process_invoice_ocr', bind=True, max_retries=3)
def process_invoice_ocr(self, invoice_id: int):
    """
    Fatura dosyasƒ±nƒ± Google Cloud Vision ile OCR i≈üleme
    
    Adƒ±mlar:
    1. Fatura dosyasƒ±nƒ± diskten oku
    2. Google Cloud Vision API ile metin √ßƒ±kar
    3. Okunan metni parse et (t√ºketim, maliyet, tarih)
    4. Invoice kaydƒ±nƒ± g√ºncelle
    5. Kullanƒ±cƒ±ya bildirim g√∂nder
    6. Hata durumunda retry yap (max 3)
    """
    db = SessionLocal()
    try:
        logger.info(f"üîç Fatura OCR i≈ülemi ba≈üladƒ±: Invoice #{invoice_id}")
        
        # Fatura kaydƒ±nƒ± bul
        invoice = db.query(models.Invoice).filter(
            models.Invoice.id == invoice_id
        ).first()
        
        if not invoice:
            logger.error(f"‚ùå Fatura bulunamadƒ±: #{invoice_id}")
            return {"status": "failed", "reason": "invoice_not_found"}
        
        # Durumu g√ºncelle
        invoice.status = models.InvoiceStatus.processing
        db.commit()
        
        # OCR ƒ∞≈ülemi - Google Cloud Vision kullanarak
        try:
            from services.invoice_ocr_service import get_ocr_service
            
            ocr_service = get_ocr_service()
            extracted_data = ocr_service.process_invoice(invoice.file_path)
            
            if not extracted_data:
                raise Exception("OCR ba≈üarƒ±sƒ±z - veri √ßƒ±karƒ±lamadƒ±")
            
            # Okunan verileri kaydet
            invoice.extracted_activity_type = extracted_data.get('activity_type')
            invoice.extracted_quantity = extracted_data.get('quantity')
            invoice.extracted_cost_tl = extracted_data.get('cost_tl')
            invoice.extracted_start_date = extracted_data.get('start_date')
            invoice.extracted_end_date = extracted_data.get('end_date')
            invoice.extracted_text = extracted_data.get('extracted_text')
            invoice.status = models.InvoiceStatus.completed
            invoice.processed_at = datetime.now().date()
            
            db.commit()
            
            logger.info(
                f"‚úÖ Fatura OCR ba≈üarƒ±lƒ±: "
                f"Type={invoice.extracted_activity_type}, "
                f"Qty={invoice.extracted_quantity}, "
                f"Cost={invoice.extracted_cost_tl} TL, "
                f"G√ºven: {extracted_data.get('confidence', 0):.0%}"
            )
            
            # Kullanƒ±cƒ±ya bildirim g√∂nder
            try:
                from services.notification_service import get_notification_service
                notif_service = get_notification_service()
                
                facility = db.query(models.Facility).filter(
                    models.Facility.id == invoice.facility_id
                ).first()
                
                if facility and facility.company:
                    # G√ºven skoru d√º≈ü√ºkse uyarƒ± ekle
                    confidence = extracted_data.get('confidence', 0)
                    confidence_warning = " ‚ö†Ô∏è (D√º≈ü√ºk g√ºven skoru - l√ºtfen kontrol edin)" if confidence < 0.6 else ""
                    
                    notif_service.create_notification(
                        db=db,
                        user_id=invoice.user_id,
                        notification_type='invoice_processed',
                        title="üìÑ Faturanƒ±z ƒ∞≈ülendi!",
                        message=f"{invoice.filename}: {invoice.extracted_quantity} {invoice.extracted_activity_type}, "
                                f"{invoice.extracted_cost_tl:.0f} TL{confidence_warning}. L√ºtfen doƒürulayƒ±n.",
                        company_id=facility.company_id,
                        facility_id=facility.id,
                        action_url=f"/dashboard/invoices/{invoice_id}/verify",
                        send_email=True
                    )
            except Exception as e:
                logger.error(f"‚ö†Ô∏è OCR sonu√ß bildirimi g√∂nderilirken hata: {e}")
            
            return {
                "status": "success",
                "invoice_id": invoice_id,
                "extracted_data": extracted_data
            }
            
        except Exception as ocr_error:
            logger.error(f"‚ùå OCR i≈üleme hatasƒ±: {ocr_error}")
            
            # Retry mekanizmasƒ±
            if self.request.retries < self.max_retries:
                logger.warning(f"üîÑ Retry {self.request.retries + 1}/{self.max_retries} ba≈üladƒ±...")
                raise self.retry(exc=ocr_error, countdown=60)  # 60 saniye sonra retry
            else:
                # Final attempt ba≈üarƒ±sƒ±z
                invoice.status = models.InvoiceStatus.failed
                invoice.processed_at = datetime.now().date()
                db.commit()
                
                logger.error(f"‚ùå OCR i≈ülemi nihayet ba≈üarƒ±sƒ±z (3 retry sonrasƒ±): {ocr_error}")
                
                return {
                    "status": "failed",
                    "reason": str(ocr_error),
                    "invoice_id": invoice_id
                }
    
    except Exception as e:
        logger.error(f"‚ùå Fatura i≈üleme g√∂revi hatasƒ±: {e}")
        invoice.status = models.InvoiceStatus.failed
        db.commit()
        return {"status": "error", "reason": str(e)}


def extract_invoice_data_with_ocr(file_path: str) -> dict:
    """
    Google Cloud Vision API ile fatura'dan veri √ßƒ±kar
    
    Mock versiyonu - ger√ßek OCR entegrasyonu yapƒ±lacak
    """
    try:
        import os
        from google.cloud import vision
        
        client = vision.ImageAnnotatorClient()
        
        # Dosyayƒ± oku
        with open(file_path, 'rb') as image_file:
            content = image_file.read()
        
        image = vision.Image(content=content)
        
        # OCR yap
        response = client.document_text_detection(image=image)
        text = response.full_text_annotation.text
        
        # Metin analizi - T√ºrk√ße fatura √∂r√ºnt√ºleri
        extracted = {
            'raw_text': text,
            'activity_type': extract_activity_type(text),
            'quantity': extract_quantity(text),
            'cost_tl': extract_cost(text),
            'start_date': extract_date(text, 'start'),
            'end_date': extract_date(text, 'end')
        }
        
        return extracted
        
    except ImportError:
        logger.warning("‚ö†Ô∏è Google Cloud Vision SDK y√ºkl√º deƒüil, mock OCR kullanƒ±lƒ±yor")
        return extract_invoice_data_mock()
    except Exception as e:
        logger.error(f"‚ùå OCR hatasƒ±: {e}")
        raise


def extract_invoice_data_mock() -> dict:
    """
    Mock OCR sonucu - geli≈ütirme i√ßin
    """
    from datetime import date, timedelta
    return {
        'raw_text': '[Mock OCR Output]',
        'activity_type': 'electricity',
        'quantity': 5000.0,
        'cost_tl': 15000.0,
        'start_date': date.today() - timedelta(days=30),
        'end_date': date.today()
    }


def extract_activity_type(text: str) -> str:
    """Metinden enerji tipi √ßƒ±kar"""
    text_lower = text.lower()
    if 'elektrik' in text_lower or 'kwh' in text_lower:
        return 'electricity'
    elif 'doƒüalgaz' in text_lower or 'gaz' in text_lower or 'm¬≥' in text_lower:
        return 'natural_gas'
    elif 'yakƒ±t' in text_lower or 'diesel' in text_lower or 'benzin' in text_lower:
        return 'diesel_fuel'
    return 'electricity'


def extract_quantity(text: str) -> float:
    """Metinden t√ºketim miktarƒ±nƒ± √ßƒ±kar"""
    import re
    # T√ºrk√ße fatura yapƒ±sƒ±ndan √∂rnek: "T√ºketim: 5.234 kWh"
    pattern = r'[tT]√ºketim\s*[:Ôºö]\s*([\d\.,]+)'
    match = re.search(pattern, text)
    if match:
        qty_str = match.group(1).replace(',', '.')
        try:
            return float(qty_str)
        except:
            pass
    return 0.0


def extract_cost(text: str) -> float:
    """Metinden fatura tutarƒ±nƒ± √ßƒ±kar"""
    import re
    # T√ºrk√ße fatura yapƒ±sƒ±ndan: "Toplam: 15.250,50 TL"
    pattern = r'[tT]oplam\s*[:Ôºö]\s*([\d\.,]+)'
    match = re.search(pattern, text)
    if match:
        cost_str = match.group(1).replace('.', '').replace(',', '.')
        try:
            return float(cost_str)
        except:
            pass
    return 0.0


def extract_date(text: str, date_type: str = 'start') -> None:
    """Metinden tarihi √ßƒ±kar"""
    import re
    from datetime import datetime
    
    # T√ºrk√ße tarih desenleri: "01.01.2024", "01/01/2024"
    pattern = r'(\d{1,2})[./](\d{1,2})[./](\d{4})'
    matches = re.findall(pattern, text)
    
    if matches:
        try:
            if date_type == 'start':
                day, month, year = matches[0]
            else:
                day, month, year = matches[-1] if len(matches) > 1 else matches[0]
            
            return datetime(int(year), int(month), int(day)).date()
        except:
            pass
    
    return None


# YENƒ∞: Asenkron Rapor √úretimi (Mod√ºl 2.1)

@app.task(name='tasks.generate_cbam_report_async', bind=True, max_retries=3)
def generate_cbam_report_async(self, report_id: int):
    """
    CBAM XML raporunu asenkron olarak √ºret
    
    Adƒ±mlar:
    1. Report kaydƒ±nƒ± bul
    2. ≈ûirketin verilerini topla
    3. XML'i olu≈ütur
    4. Dosyayƒ± kaydet (S3 veya local)
    5. Report status'√º g√ºncelle
    6. Bildirim g√∂nder
    """
    db = SessionLocal()
    try:
        logger.info(f"üìä CBAM raporu olu≈üturuluyor: Report #{report_id}")
        
        # Report kaydƒ±nƒ± bul
        report = db.query(models.Report).filter(
            models.Report.id == report_id
        ).first()
        
        if not report:
            logger.error(f"‚ùå Rapor bulunamadƒ±: #{report_id}")
            return {"status": "failed", "reason": "report_not_found"}
        
        # Status'√º g√ºncelle
        report.status = models.ReportStatus.processing
        report.requested_at = datetime.utcnow()
        db.commit()
        
        # CBAM servisi ile rapor olu≈ütur
        try:
            from services.cbam_service import CBAMReportService
            cbam_service = CBAMReportService()
            
            xml_content = cbam_service.generate_cbam_report(
                company_id=report.company_id,
                start_date=report.start_date,
                end_date=report.end_date,
                reporting_period=report.period_name
            )
            
            # Dosyayƒ± kaydet
            import os
            import uuid
            
            report_dir = "/tmp/reports"
            os.makedirs(report_dir, exist_ok=True)
            
            filename = f"cbam_{report.company_id}_{uuid.uuid4()}.xml"
            file_path = os.path.join(report_dir, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(xml_content)
            
            file_size = os.path.getsize(file_path)
            
            # Report kaydƒ±nƒ± g√ºncelle
            report.status = models.ReportStatus.completed
            report.file_path = file_path
            report.file_size_bytes = file_size
            report.completed_at = datetime.utcnow()
            report.expires_at = datetime.utcnow() + timedelta(days=7)  # 7 g√ºnl√ºk TTL
            
            db.commit()
            
            logger.info(
                f"‚úÖ CBAM raporu olu≈üturuldu: {filename} ({file_size} bytes)"
            )
            
            # Bildirim g√∂nder
            if report.notify_user_when_ready:
                try:
                    from services.notification_service import get_notification_service
                    notif_service = get_notification_service()
                    
                    company = db.query(models.Company).filter(
                        models.Company.id == report.company_id
                    ).first()
                    
                    if company:
                        notif_service.create_notification(
                            db=db,
                            user_id=report.user_id,
                            notification_type='report_ready',
                            title="üìä CBAM Raporunuz Hazƒ±r!",
                            message=f"{company.name} i√ßin {report.period_name or 'belirtilen d√∂nem'} CBAM XML raporu hazƒ±rlanmƒ±≈ütƒ±r. ƒ∞ndirmek i√ßin tƒ±klayƒ±n.",
                            company_id=report.company_id,
                            action_url=f"/dashboard/reports/{report_id}/download",
                            send_email=True
                        )
                except Exception as e:
                    logger.error(f"‚ö†Ô∏è Rapor hazƒ±r bildirimi g√∂nderilemedi: {e}")
            
            return {
                "status": "success",
                "report_id": report_id,
                "file_path": file_path,
                "file_size": file_size
            }
            
        except Exception as cbam_error:
            logger.error(f"‚ùå CBAM √ºretimi hatasƒ±: {cbam_error}")
            report.status = models.ReportStatus.failed
            report.error_message = str(cbam_error)
            report.error_trace = str(cbam_error.__traceback__)
            db.commit()
            
            # Retry
            raise self.retry(exc=cbam_error, countdown=300)
        
    except Exception as exc:
        logger.error(f"‚ùå CBAM rapor g√∂rev hatasƒ±: {exc}")
        if report:
            report.status = models.ReportStatus.failed
            report.error_message = str(exc)
            db.commit()
        raise self.retry(exc=exc, countdown=600)
        
    finally:
        db.close()


@app.task(name='tasks.calculate_roi_analysis_async', bind=True, max_retries=3)
def calculate_roi_analysis_async(self, report_id: int):
    """
    ROI analiz raporunu asenkron olarak hesapla ve PDF'e d√∂n√º≈üt√ºr
    
    Adƒ±mlar:
    1. Report kaydƒ±nƒ± bul
    2. ROI hesaplamasƒ±nƒ± yap
    3. Sonu√ßlarƒ± JSON olarak kaydet
    4. PDF'e d√∂n√º≈üt√ºr (opsiyonel - ≈üimdi JSON)
    5. Bildirim g√∂nder
    """
    db = SessionLocal()
    try:
        logger.info(f"üí∞ ROI analiz raporu olu≈üturuluyor: Report #{report_id}")
        
        # Report kaydƒ±nƒ± bul
        report = db.query(models.Report).filter(
            models.Report.id == report_id
        ).first()
        
        if not report:
            logger.error(f"‚ùå Rapor bulunamadƒ±: #{report_id}")
            return {"status": "failed", "reason": "report_not_found"}
        
        # Status'√º g√ºncelle
        report.status = models.ReportStatus.processing
        report.requested_at = datetime.utcnow()
        db.commit()
        
        # ROI hesapla
        try:
            from services.roi_calculator_service import ROICalculatorService
            roi_service = ROICalculatorService(db)
            
            # Analiz s√ºresini hesapla (d√∂neme g√∂re)
            period_months = 12
            roi_analysis = roi_service.calculate_roi_potential(
                company_id=report.company_id,
                period_months=period_months
            )
            
            # Sonu√ßlarƒ± JSON olarak kaydet
            import json
            import os
            import uuid
            
            report_dir = "/tmp/reports"
            os.makedirs(report_dir, exist_ok=True)
            
            filename = f"roi_{report.company_id}_{uuid.uuid4()}.json"
            file_path = os.path.join(report_dir, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(roi_analysis.dict(), f, ensure_ascii=False, indent=2, default=str)
            
            file_size = os.path.getsize(file_path)
            
            # Report kaydƒ±nƒ± g√ºncelle
            report.status = models.ReportStatus.completed
            report.file_path = file_path
            report.file_size_bytes = file_size
            report.total_savings_tl = roi_analysis.potential_annual_savings_tl
            report.completed_at = datetime.utcnow()
            report.expires_at = datetime.utcnow() + timedelta(days=7)
            
            db.commit()
            
            logger.info(
                f"‚úÖ ROI raporu olu≈üturuldu: {filename} "
                f"(Tasarruf: {roi_analysis.potential_annual_savings_tl:.0f} TL)"
            )
            
            # Bildirim g√∂nder
            if report.notify_user_when_ready:
                try:
                    from services.notification_service import get_notification_service
                    notif_service = get_notification_service()
                    
                    company = db.query(models.Company).filter(
                        models.Company.id == report.company_id
                    ).first()
                    
                    if company:
                        notif_service.create_notification(
                            db=db,
                            user_id=report.user_id,
                            notification_type='report_ready',
                            title="üí∞ ROI Analiz Raporunuz Hazƒ±r!",
                            message=f"{company.name} i√ßin yƒ±llƒ±k {roi_analysis.potential_annual_savings_tl:.0f} TL tasarruf potansiyeli tespit edildi! Detaylƒ± analiz i√ßin tƒ±klayƒ±n.",
                            company_id=report.company_id,
                            action_url=f"/dashboard/reports/{report_id}/view",
                            send_email=True
                        )
                except Exception as e:
                    logger.error(f"‚ö†Ô∏è ROI rapor bildirim g√∂nderilemedi: {e}")
            
            return {
                "status": "success",
                "report_id": report_id,
                "file_path": file_path,
                "file_size": file_size,
                "total_savings_tl": roi_analysis.potential_annual_savings_tl
            }
            
        except Exception as roi_error:
            logger.error(f"‚ùå ROI hesaplama hatasƒ±: {roi_error}")
            report.status = models.ReportStatus.failed
            report.error_message = str(roi_error)
            db.commit()
            
            raise self.retry(exc=roi_error, countdown=300)
        
    except Exception as exc:
        logger.error(f"‚ùå ROI rapor g√∂rev hatasƒ±: {exc}")
        if report:
            report.status = models.ReportStatus.failed
            report.error_message = str(exc)
            db.commit()
        raise self.retry(exc=exc, countdown=600)
        
    finally:
        db.close()


@app.task(name='tasks.cleanup_expired_reports', bind=True)
def cleanup_expired_reports(self):
    """
    S√ºresi dolmu≈ü raporlarƒ± sil (g√ºnde bir √ßalƒ±≈üan)
    """
    db = SessionLocal()
    try:
        logger.info("üßπ S√ºresi dolmu≈ü raporlar temizleniyor...")
        
        expired_reports = db.query(models.Report).filter(
            models.Report.expires_at <= datetime.utcnow(),
            models.Report.status != models.ReportStatus.expired
        ).all()
        
        deleted_count = 0
        for report in expired_reports:
            try:
                # Dosyayƒ± sil
                if report.file_path and os.path.exists(report.file_path):
                    os.remove(report.file_path)
                    logger.debug(f"üìÅ Dosya silindi: {report.file_path}")
                
                # Report kaydƒ±nƒ± sil veya i≈üaretle
                report.status = models.ReportStatus.expired
                deleted_count += 1
                
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rapor temizleme hatasƒ±: {e}")
        
        db.commit()
        logger.info(f"‚úÖ {deleted_count} s√ºresi dolmu≈ü rapor temizlendi")
        
        return {"cleaned_count": deleted_count}
        
    except Exception as exc:
        logger.error(f"‚ùå Temizlik g√∂rev hatasƒ±: {exc}")
    finally:
        db.close()


@app.task(name='tasks.calculate_supplier_benchmarks', bind=True, max_retries=3)
def calculate_supplier_benchmarks(self):
    """
    Tedarik√ßi √ºr√ºnleri i√ßin sekt√∂rel benchmark'larƒ± hesapla
    
    Her product_category i√ßin:
    - Ortalama co2e_per_unit_kg
    - Medyan deƒüer
    - En iyi %25'lik dilim (best_in_class)
    - Toplam √ºr√ºn sayƒ±sƒ±
    
    Bu veriler tedarik√ßilere kendi performanslarƒ±nƒ± 
    sekt√∂r ortalamasƒ±yla kar≈üƒ±la≈ütƒ±rma imkanƒ± saƒülar.
    """
    db = SessionLocal()
    try:
        logger.info("üîÑ Tedarik√ßi benchmark hesaplama ba≈üladƒ±...")
        
        # T√ºm unique product_category'leri al
        categories = db.query(models.ProductFootprint.product_category).distinct().all()
        
        benchmark_results = {}
        
        for (category,) in categories:
            if not category:
                continue
            
            # Bu kategorideki t√ºm √ºr√ºnleri al
            products = db.query(models.ProductFootprint).filter(
                models.ProductFootprint.product_category == category,
                models.ProductFootprint.co2e_per_unit_kg > 0  # Sadece ge√ßerli deƒüerler
            ).all()
            
            if not products:
                continue
            
            # CO2e deƒüerlerini topla
            co2e_values = [p.co2e_per_unit_kg for p in products]
            
            if not co2e_values:
                continue
            
            # ƒ∞statistikleri hesapla
            avg_co2e = sum(co2e_values) / len(co2e_values)
            
            # Medyan hesaplama
            sorted_values = sorted(co2e_values)
            mid = len(sorted_values) // 2
            median_co2e = (sorted_values[mid] + sorted_values[~mid]) / 2 if len(sorted_values) > 0 else 0
            
            # En iyi %25'lik dilim (best_in_class) - en d√º≈ü√ºk emisyonlar
            percentile_25_index = int(len(sorted_values) * 0.25)
            best_in_class = sorted_values[percentile_25_index] if percentile_25_index < len(sorted_values) else sorted_values[0]
            
            benchmark_results[category] = {
                "category": category,
                "avg_co2e_per_unit": round(avg_co2e, 3),
                "median_co2e_per_unit": round(median_co2e, 3),
                "best_in_class": round(best_in_class, 3),
                "product_count": len(products),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            logger.info(
                f"üìä {category}: Ort={avg_co2e:.2f}, Medyan={median_co2e:.2f}, "
                f"Best={best_in_class:.2f} kg CO2e ({len(products)} √ºr√ºn)"
            )
        
        # Sonu√ßlarƒ± cache'e kaydet (Redis veya DB'ye kaydedilebilir)
        # ≈ûu an i√ßin sadece log'layalƒ±m, gelecekte Redis'e kaydedilecek
        
        logger.info(f"‚úÖ {len(benchmark_results)} kategori i√ßin benchmark hesaplandƒ±")
        
        return {
            "success": True,
            "categories_processed": len(benchmark_results),
            "benchmarks": benchmark_results
        }
        
    except Exception as exc:
        logger.error(f"‚ùå Benchmark hesaplama hatasƒ±: {exc}")
        raise self.retry(exc=exc, countdown=60)  # 1 dakika sonra tekrar dene
    finally:
        db.close()
